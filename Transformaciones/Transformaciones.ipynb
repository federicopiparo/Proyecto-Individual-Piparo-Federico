{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Transformaciones\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.read_csv('C:\\\\Users\\\\fede\\\\Desktop\\\\LABs 1\\\\Proyecto individual - Federico Piparo\\\\Transformaciones\\\\Movies\\\\movies_dataset.csv', low_memory=False)\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\fede\\\\Desktop\\\\LABs 1\\\\Proyecto individual - Federico Piparo\\\\Transformaciones\\\\Movies\\\\credits.csv', low_memory=False)\n",
    "\n",
    "#'''trabajaré al mismo tiempo con el DF Credits, ya que algunas funciones de la API necesitan datos provenientes de este'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#los convertimos a 'parquet' para optimizar el rendimineto\n",
    "df.to_parquet('data_compressed.parquet', engine='pyarrow', compression='snappy')\n",
    "# Guardar con compresión snappy\n",
    "df2.to_parquet('data_compressed.parquet', engine='pyarrow', compression='snappy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas duplicadas eliminadas: 17\n",
      "Número de filas duplicadas eliminadas en el segundo DF: 37\n"
     ]
    }
   ],
   "source": [
    "# Contar el número de filas duplicadas\n",
    "duplicados = df.duplicated().sum()\n",
    "\n",
    "# Eliminar filas duplicadas\n",
    "df_sin_duplicados = df.drop_duplicates()\n",
    "\n",
    "# Almacenar el número de filas duplicadas en una variable, esto simplemente para verificar que si existían filas duplicadas\n",
    "numero_de_duplicados = duplicados\n",
    "\n",
    "print(\"Número de filas duplicadas eliminadas:\", numero_de_duplicados)\n",
    "\n",
    "#segundo DF\n",
    "# Contar el número de filas duplicadas \n",
    "duplicados = df2.duplicated().sum()\n",
    "\n",
    "# Eliminar filas duplicadas\n",
    "df_sin_duplicados = df2.drop_duplicates()\n",
    "\n",
    "# Almacenar el número de filas duplicadas en una variable, esto simplemente para verificar que si existían filas duplicadas\n",
    "numero_de_duplicados = duplicados\n",
    "print(\"Número de filas duplicadas eliminadas en el segundo DF:\", numero_de_duplicados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Eliminar las columnas que no serán utilizadas, video,imdb_id,adult,original_title,poster_path y homepage.'''\n",
    "df = df.drop(columns=['video', 'imdb_id', 'adult', 'original_title', 'poster_path', 'homepage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Los valores nulos del campo release date deben eliminarse.'''\n",
    "\n",
    "df = df.dropna(subset=['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Los valores nulos de los campos revenue, budget deben ser rellenados por el número 0.'''\n",
    "\n",
    "df[['revenue', 'budget']] = df[['revenue', 'budget']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''De haber fechas, deberán tener el formato AAAA-mm-dd, además deberán crear la columna release_year donde extraerán el año de la fecha de estreno.'''\n",
    "\n",
    "#convertimos las fechas al datatype datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce') \n",
    "df['release_year'] = df['release_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Crear la columna con el retorno de inversión, llamada return con los campos revenue y budget, dividiendo estas dos últimas revenue / budget,\n",
    "cuando no hay datos disponibles para calcularlo, deberá tomar el valor 0.'''\n",
    "# Convertir las columnas 'revenue' y 'budget' a tipo numérico\n",
    "df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce')\n",
    "df['budget'] = pd.to_numeric(df['budget'], errors='coerce')\n",
    "# Rellenar valores nulos en 'revenue' y 'budget' con 0\n",
    "df['revenue'] = df['revenue'].fillna(0)\n",
    "df['budget'] = df['budget'].fillna(0)\n",
    "\n",
    "# Crear la columna 'return' calculando revenue / budget\n",
    "# Usar np.where para evitar división por 0\n",
    "import numpy as np\n",
    "df['return'] = np.where(df['budget'] == 0, 0, df['revenue'] / df['budget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fede\\AppData\\Local\\Temp\\ipykernel_12724\\1652706673.py:21: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  BTC_df = df['belongs_to_collection'].apply(pd.Series)\n",
      "C:\\Users\\fede\\AppData\\Local\\Temp\\ipykernel_12724\\1652706673.py:27: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  genres_df = df['genres'].apply(pd.Series)\n",
      "C:\\Users\\fede\\AppData\\Local\\Temp\\ipykernel_12724\\1652706673.py:28: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  production_companies_df = df['production_companies'].apply(pd.Series)\n",
      "C:\\Users\\fede\\AppData\\Local\\Temp\\ipykernel_12724\\1652706673.py:29: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  production_countries_df = df['production_countries'].apply(pd.Series)\n",
      "C:\\Users\\fede\\AppData\\Local\\Temp\\ipykernel_12724\\1652706673.py:30: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  spoken_languages_df = df['spoken_languages'].apply(pd.Series)\n"
     ]
    }
   ],
   "source": [
    "''' Algunos campos, como belongs_to_collection, production_companies y otros (ver diccionario de datos) están anidados, esto es o bien tienen un diccionario o una\n",
    "lista como valores en cada fila, ¡deberán desanidarlos para poder y unirlos al dataset de nuevo hacer alguna de las consultas de la API!\n",
    "O bien buscar la manera de acceder a esos datos sin desanidarlos.'''\n",
    "# Función para convertir cadenas a diccionarios o listas\n",
    "def convertir_en_obj(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return x  # Retorna la cadena si no es un diccionario válido\n",
    "    return x\n",
    "\n",
    "# Aplicar la conversión a las series \n",
    "df['belongs_to_collection'] = df['belongs_to_collection'].apply(convertir_en_obj)\n",
    "df['genres'] = df['genres'].apply(convertir_en_obj)\n",
    "df['production_companies'] = df['production_companies'].apply(convertir_en_obj)\n",
    "df['production_countries'] = df['production_countries'].apply(convertir_en_obj)\n",
    "df['spoken_languages'] = df['spoken_languages'].apply(convertir_en_obj)\n",
    "\n",
    "# Desanidar diccionarios\n",
    "BTC_df = df['belongs_to_collection'].apply(pd.Series)\n",
    "BTC_df = BTC_df.drop(columns = 0, errors='ignore')  # Eliminar columna de índice si existe\n",
    "nuevos_nombres_BTC = ['BTCid', 'BTCname', 'BTCposter_path', 'BTCbackdrop_path']\n",
    "BTC_df.columns = nuevos_nombres_BTC\n",
    "\n",
    "# Desanidar listas\n",
    "genres_df = df['genres'].apply(pd.Series)\n",
    "production_companies_df = df['production_companies'].apply(pd.Series)\n",
    "production_countries_df = df['production_countries'].apply(pd.Series)\n",
    "spoken_languages_df = df['spoken_languages'].apply(pd.Series)\n",
    "\n",
    "# Asegurarse de que no haya nombres de columnas duplicados\n",
    "genres_df = genres_df.rename(columns=lambda x: f'genre_{x}')\n",
    "production_companies_df = production_companies_df.rename(columns=lambda x: f'company_{x}')\n",
    "production_countries_df = production_countries_df.rename(columns=lambda x: f'country_{x}')\n",
    "spoken_languages_df = spoken_languages_df.rename(columns=lambda x: f'language_{x}')\n",
    "\n",
    "# Eliminar las columnas originales\n",
    "df = df.drop(columns=['belongs_to_collection', 'genres', 'production_companies', 'production_countries', 'spoken_languages'])\n",
    "\n",
    "# Unir los DataFrames desanidado al DataFrame original\n",
    "df = df.join(BTC_df)\n",
    "df = df.join(production_companies_df)\n",
    "df = df.join(production_countries_df)\n",
    "df = df.join(spoken_languages_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Transformaciones sobre credits (API)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Al ser un DF muy grande y necesitar solo partes muy especificas de los diccionarios que componen las series del DF,\n",
    "en vez de desanidarlo todo, utilizaremos otras tecnicas para extraer los datos que necesitamos, y con estos crearemos una nueva tabla,\n",
    "esta misma contendra solamente los datos que necesitamos para las funciones de nuestra API '''\n",
    "\n",
    "# Extraer los nombres de la columna 'cast'\n",
    "names = df2['cast'].apply(lambda x: [item['name'] for item in ast.literal_eval(x) if 'name' in item])\n",
    "\n",
    "# Convertir la serie 'names' en un DataFrame\n",
    "names_df = pd.DataFrame(names.tolist())\n",
    "\n",
    "# Añadir la palabra 'nombre' delante del nombre de cada serie\n",
    "names_df.columns = ['nombre_' + str(col) for col in names_df.columns]\n",
    "\n",
    "# Crear una serie que contenga únicamente el nombre del director\n",
    "names_df['Nombre_Director'] = df2['crew'].apply(\n",
    "    lambda x: next((item['name'] for item in ast.literal_eval(x) if item.get('job') == 'Director'), None)\n",
    ")\n",
    "\n",
    "# volvemos a añadir los ID a las filas, ya que los necesitaremos luego para hacer un \"Join\"\n",
    "names_df['id'] = df2['id'].astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = pd.to_numeric(df['id'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Hacemos un \"Join\" entre ambos DF\n",
    "df = pd.merge(names_df,df, on='id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exportación del Datafame\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('transformados.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Transformaciones para el sistema de recomendación (machine learning)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "#estas transformaciones las hago ya habiendo casi finalizado el trabajo, para el sistema de recomendación\n",
    "#He decidido que para este crearé un nuevo DF, el cual será una versión reducida del mismo, y que mantendrá unicamente las series que considere pertinentes\n",
    "\n",
    "df_ML = pd.read_parquet (\"C:\\\\Users\\\\fede\\\\Desktop\\\\LABs 1\\\\Proyecto individual - Federico Piparo\\\\Transformaciones\\\\transformados.parquet\")\n",
    "\n",
    "#Por motivos de simplificación del modelo de ML deberé usar unicamente variables numéricas, teniendo que eliminar las demás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahora me quedaré unicamente con las series que utilizaré para mi sistema de recomendación.\n",
    "columnas_deseadas = ['title', 'popularity', 'runtime', 'vote_average']\n",
    "df_ML = df_ML[columnas_deseadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ML = df_ML.drop_duplicates(subset='title', keep='first')\n",
    "\n",
    "# Convertir la columna 'popularity' a float64\n",
    "df_ML['popularity'] = pd.to_numeric(df_ML['popularity'], errors='coerce')\n",
    "\n",
    "# Crear una columna que cuenta el número de ceros y NaNs por fila\n",
    "df_ML['count_zeros_nans'] = (df_ML == 0).sum(axis=1) + df_ML.isna().sum(axis=1)\n",
    "\n",
    "# Ordenar el DataFrame por esta nueva columna en orden descendente\n",
    "df_ML = df_ML.sort_values(by='count_zeros_nans', ascending=False)\n",
    "\n",
    "# Eliminar la mitad de las filas con más valores 0 o NaN\n",
    "df_ML = df_ML.iloc[len(df_ML) // 2:]\n",
    "\n",
    "# Eliminar la columna auxiliar 'count_zeros_nans'\n",
    "df_ML = df_ML.drop(columns=['count_zeros_nans'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar 0 por NaN en todas las columnas excepto 'title'\n",
    "df_ML.loc[:, df_ML.columns != 'title'] = df_ML.loc[:, df_ML.columns != 'title'].replace(0, np.nan)\n",
    "\n",
    "# Imputar NaNs con el promedio de cada columna (excepto 'title')\n",
    "df_ML.loc[:, df_ML.columns != 'title'] = df_ML.loc[:, df_ML.columns != 'title'].fillna(df_ML.mean(numeric_only=True))\n",
    "\n",
    "# Reemplazar los ceros originales con los valores imputados\n",
    "df_ML.loc[:, df_ML.columns != 'title'] = df_ML.loc[:, df_ML.columns != 'title'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraer características numéricas para las recomendaciones\n",
    "features = df_ML[['popularity', 'runtime', 'vote_average']]\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Calcular similitudes usando el coseno\n",
    "cosine_sim = cosine_similarity(features_scaled)\n",
    "\n",
    "# Guardar la matriz de similitud en un archivo para no tener que calcularlo en nuestra API \n",
    "np.save('similitud_del_coseno.npy', cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos a necesitar un DF con los titulos de las peliculas para la función\n",
    "df = pd.read_parquet (\"C:\\\\Users\\\\fede\\\\Desktop\\\\LABs 1\\\\Proyecto individual - Federico Piparo\\\\Transformaciones\\\\transformados.parquet\")\n",
    "dftitulos = df['title']\n",
    "# Convierte la Serie a un DataFrame\n",
    "dftitulos = dftitulos.to_frame()\n",
    "dftitulos.to_parquet('df_titulos.parquet',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
